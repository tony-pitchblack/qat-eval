{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class NextItemDataset(Dataset):\n",
        "    def __init__(self, root_dir: str, dataset: str, split: str, min_len: int = 1):\n",
        "        name_map = {\n",
        "            \"Dunnhumby\": (\"Dunnhumby_history.csv\", \"Dunnhumby_future.csv\"),\n",
        "            \"Instacart\": (\"Instacart_history.csv\", \"Instacart_future.csv\"),\n",
        "            \"TaFang\": (\"TaFang_history_NB.csv\", \"TaFang_future_NB.csv\"),\n",
        "            \"ValuedShopper\": (\"VS_history_order.csv\", \"VS_future_order.csv\"),\n",
        "        }\n",
        "        if dataset not in name_map:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "        if split not in {\"train\", \"val\"}:\n",
        "            raise ValueError(\"split must be 'train' or 'val'\")\n",
        "        filename = name_map[dataset][0 if split == \"train\" else 1]\n",
        "        path = os.path.join(root_dir, filename)\n",
        "\n",
        "        dtypes = {\"CUSTOMER_ID\": \"int64\", \"ORDER_NUMBER\": \"int64\", \"MATERIAL_NUMBER\": \"int64\"}\n",
        "        df = pd.read_csv(path, dtype=dtypes)\n",
        "        df = df.sort_values([\"CUSTOMER_ID\", \"ORDER_NUMBER\"])  # stable within order\n",
        "        grouped = df.groupby(\"CUSTOMER_ID\")[\"MATERIAL_NUMBER\"].apply(list)\n",
        "        self.sequences: List[torch.Tensor] = [\n",
        "            torch.tensor(seq, dtype=torch.long) for seq in grouped.tolist() if len(seq) >= min_len\n",
        "        ]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index: int) -> torch.Tensor:\n",
        "        return self.sequences[index]\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    lengths = torch.tensor([len(x) for x in batch], dtype=torch.long)\n",
        "    padded = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    return padded, lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = \"../external_repos/TIFUKNN/data\"\n",
        "DATASET_NAME = \"Dunnhumby\"  # change to one of: Dunnhumby, Instacart, TaFang, ValuedShopper\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = NextItemDataset(root_dir=DATA_DIR, dataset=DATASET_NAME, split=\"train\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_ds = NextItemDataset(root_dir=DATA_DIR, dataset=DATASET_NAME, split=\"val\")\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 73]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "batch, lengths = next(iter(train_loader))\n",
        "print(batch.shape, lengths.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/a1111/micromamba/envs/qat-eval/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(\"..\"))  # make project root importable\n",
        "\n",
        "from models.sasrec import SASRecModel\n",
        "model = SASRecModel(num_items=5000, max_seq_len=50, hidden_size=64, num_heads=1, num_layers=1, dropout=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2089, -0.2415,  0.2061,  ..., -0.8096, -0.6885, -0.3644],\n",
              "        [-0.5335,  0.4761,  1.1201,  ...,  0.1036,  0.1481, -0.4338],\n",
              "        [-0.2751,  1.2457,  0.2522,  ...,  0.3616, -0.0234,  0.2877],\n",
              "        ...,\n",
              "        [-0.5629, -0.5497, -1.3004,  ...,  0.4144,  0.1152,  0.8768],\n",
              "        [-1.0638, -0.7551,  0.1086,  ..., -0.0692, -1.0627, -0.1640],\n",
              "        [ 0.0054, -0.0542,  1.3416,  ...,  0.8523,  0.3845,  0.1576]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = model(batch, lengths)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:02<00:00,  9.06it/s, loss=7.33]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Loss: 7.2519 - Train NDCG@10: 0.0810 - Val NDCG@10: 0.0720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:08<00:00,  8.29it/s, loss=6.54]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Loss: 6.8846 - Train NDCG@10: 0.0885 - Val NDCG@10: 0.0739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [06:08<00:00,  1.54it/s, loss=6.47]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Loss: 6.5230 - Train NDCG@10: 0.0975 - Val NDCG@10: 0.0735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [03:42<00:00,  2.55it/s, loss=6.01]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Loss: 6.1277 - Train NDCG@10: 0.1165 - Val NDCG@10: 0.0693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [03:15<00:00,  2.91it/s, loss=5.92]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Loss: 5.7687 - Train NDCG@10: 0.1456 - Val NDCG@10: 0.0677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:14<00:00,  7.63it/s, loss=5.44]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Loss: 5.4776 - Train NDCG@10: 0.1751 - Val NDCG@10: 0.0676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:09<00:00,  8.19it/s, loss=5.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Loss: 5.2433 - Train NDCG@10: 0.1976 - Val NDCG@10: 0.0628\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:20<00:00,  7.06it/s, loss=5.52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Loss: 5.0580 - Train NDCG@10: 0.2185 - Val NDCG@10: 0.0602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:14<00:00,  7.62it/s, loss=5.77]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Loss: 4.9040 - Train NDCG@10: 0.2339 - Val NDCG@10: 0.0601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 567/567 [01:01<00:00,  9.15it/s, loss=5.34]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Loss: 4.7665 - Train NDCG@10: 0.2494 - Val NDCG@10: 0.0600\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from metrics import ndcg_at_k\n",
        "\n",
        "def train(model, train_dataset, val_dataset, batch_size, num_epochs, metric_fn):\n",
        "    device = next(model.parameters()).device\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        train_losses = []\n",
        "        train_preds = []\n",
        "        train_targets = []\n",
        "        for batch, lengths in progress_bar:\n",
        "            batch = batch.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            bsz = batch.size(0)\n",
        "            idx = lengths - 1\n",
        "            targets = batch[torch.arange(bsz, device=device), idx]\n",
        "            inputs = batch.clone()\n",
        "            inputs[torch.arange(bsz, device=device), idx] = 0\n",
        "            seq_lens = torch.clamp(lengths - 1, min=1)\n",
        "\n",
        "            logits = model(inputs, seq_lens)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "            train_losses.append(loss.item())\n",
        "            train_preds.append(logits.detach().cpu())\n",
        "            train_targets.append(targets.detach().cpu())\n",
        "\n",
        "        train_preds_tensor = torch.cat(train_preds, dim=0)\n",
        "        train_targets_tensor = torch.cat(train_targets, dim=0)\n",
        "        train_ndcg = metric_fn(train_preds_tensor, train_targets_tensor)\n",
        "\n",
        "        # Validation with tqdm progress bar\n",
        "        model.eval()\n",
        "        val_preds, val_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            val_progress_bar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\")\n",
        "            for batch, lengths in val_progress_bar:\n",
        "                batch = batch.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                bsz = batch.size(0)\n",
        "                idx = lengths - 1\n",
        "                targets = batch[torch.arange(bsz, device=device), idx]\n",
        "                inputs = batch.clone()\n",
        "                inputs[torch.arange(bsz, device=device), idx] = 0\n",
        "                seq_lens = torch.clamp(lengths - 1, min=1)\n",
        "\n",
        "                logits = model(inputs, seq_lens)\n",
        "                val_preds.append(logits.cpu())\n",
        "                val_targets.append(targets.cpu())\n",
        "        val_preds_tensor = torch.cat(val_preds, dim=0)\n",
        "        val_targets_tensor = torch.cat(val_targets, dim=0)\n",
        "        val_ndcg = metric_fn(val_preds_tensor, val_targets_tensor)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {sum(train_losses)/len(train_losses):.4f} - Train NDCG@10: {train_ndcg:.4f} - Val NDCG@10: {val_ndcg:.4f}\")\n",
        "\n",
        "train_dataset = NextItemDataset(root_dir=DATA_DIR, dataset=DATASET_NAME, split=\"train\", min_len=2)\n",
        "val_dataset = NextItemDataset(root_dir=DATA_DIR, dataset=DATASET_NAME, split=\"val\")\n",
        "\n",
        "train(model, train_dataset, val_dataset, BATCH_SIZE, num_epochs=10, metric_fn=lambda preds, targets: ndcg_at_k(preds, targets, k=10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
